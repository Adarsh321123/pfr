\chapter{Shannon entropy inequalities}

Random variables in this paper are measurable maps $X : \Omega \to S$ from a probability space $\Omega$ to a finite set $S$ (equipped with the discrete random variable), and called $S$-valued random variables.
(NOTE: may wish to generalize to infinite $S$, but then require that $X$ almost surely ranges in a finite subset.)

\begin{definition}[Entropy]
  \label{entropy-def}
  \lean{entropy}
  \leanok
  If $X$ is an $S$-valued random variable, the entropy $H[X]$ of $X$ is defined
  $$ H[X] := \sum_{s \in S} P[X=x] \log \frac{1}{P[X=x]}$$
  with the convention that $0 \log \frac{1}{0} = 0$.
\end{definition}

\begin{lemma}[Entropy and relabeling]\label{relabeled-entropy} \uses{entropy-def}  If $X: \Omega \to S$ and $Y: \Omega \to T$ are random variables, and $Y = f(X)$ almost surely for some injection $f: S \to T$, then $H[X] = H[Y]$.
\end{lemma}

\begin{proof} Expand out both entropies and rearrange.
\end{proof}

\begin{lemma}[Jensen bound]\label{jensen-bound}
\uses{entropy-def,jensen}
\lean{entropy_le_log}
\leanok
If $X$ is an $S$-valued random variable, then $H[X] \leq \log |X|$.
\end{lemma}

\begin{proof}
  This is a direct consequence of Lemma \ref{jensen}.
\end{proof}

\begin{lemma}[Entropy of uniform random variable]\label{uniform-entropy}
  \uses{entropy-def,converse-jensen}
  \lean{entropy_of_uniform}
If $X$ is $S$-valued random variable, then $H[X] = \log |S|$ if and only if $X$ is uniformly distributed. NOTE: only one direction proved so far.
\end{lemma}

\begin{proof} Direct computation in one direction.  Converse direction needs Lemma \ref{converse-jensen}.
\end{proof}

\begin{lemma}[Bounded entropy implies concentration]\label{bound-conc}
  \uses{entropy-def}
  If $X$ is an $S$-valued random variable, then there exists $s \in S$ such that $P[X=s] \geq \exp(-H[X])$.
\end{lemma}

\begin{proof}  We have
$$ H[X] = \sum_{s \in S} P[X=s] \log \frac{1}{P[X=s]} \geq \min_{s \in S} \log \frac{1}{P[X=s]}$$
and the claim follows.
\end{proof}

\begin{definition}[Pair of random variables]\label{pair-def}
  \lean{pair}
  \leanok
Given an $S$-random variable $X: \Omega \to S$ and a $T$-valued random variable $Y: \Omega \to T$, the pair $\langle X,Y \rangle: \Omega \to S \times T$ is defined by $\langle X, Y \rangle: \omega \mapsto (X(\omega), Y(\omega))$.
\end{definition}

In this blueprint we will often drop the angled brackets from $\langle X, Y \rangle$ to reduce clutter (but they will be needed in the Lean formalization).

\begin{lemma}[Symmetry of joint entropy]\label{joint-symm}\uses{pair-def,relabeled-entropy}  If $X: \Omega \to S$ and $Y: \Omega \to T$ are random variables, then $H[X,Y ] = H[ Y,X]$.\end{lemma}

\begin{proof} Set up an injection from $\langle X,Y\rangle$ to $\langle Y,X\rangle$ and use Lemma \ref{relabeled-entropy}.
\end{proof}


\begin{definition}[Conditioned event]\label{condition-event-def}  If $X: \Omega \to S$ is an $S$-valued random variable and $E$ is an event in $\Omega$, then the conditioned event $(X|E)$ is defined to be the same random variable as $X$, but now the ambient probability measure has been conditioned to $E$.
\end{definition}

Note: it may happen that $E$ has zero measure.  In which case, the ambient probability measure should be replaced with a zero measure.  (There will be support in the project for ``probability measures with zero'' to deal with this contingency.  Fortunately, with a zero probability measure all entropies are zero, so entropy inequalities are trivially true in this case.)

\begin{definition}[Conditional entropy]\label{conditional-entropy-def}
  \uses{entropy-def,condition-event-def}  If $X: \Omega \to S$ and $Y: \Omega \to T$ are random variables, the conditional entropy $H[X|Y]$ is defined as
  $$ H[X|Y] := \sum_{y \in Y} P[Y = y] H[(X | Y=y)].$$
\end{definition}

\begin{lemma}[Chain rule]\label{chain-rule}
  \uses{conditional-entropy-def,pair-def}
  If $X: \Omega \to S$ and $Y: \Omega \to T$ are random variables, then
  $$ H[ X,Y  ] = H[Y] + H[X|Y].$$
\end{lemma}

\begin{proof} Direct computation, I guess?
\end{proof}

\begin{lemma}[Conditional chain rule]\label{conditional-chain-rule} \uses{chain-rule}
  If $X: \Omega \to S$, $Y: \Omega \to T$, $Z: \Omega \to U$ are random variables, then
$$ H[  X,Y | Z ] = H[Y | Z] + H[X|Y, Z].$$
\end{lemma}

\begin{proof}  For each $z \in U$, we can apply Lemma \ref{chain-rule} to the random variables $(X|Z=z)$ and $(Y|Z=z)$ to obtain
$$ H[ (X|Z=z),(Y|Z=z) ] = H[Y|Z=z] + H[(X|Z=z)|(Y|Z=z)].$$
Now multiply by $P[Z=z]$ and sum.  Some helper lemmas may be needed to get to the form above.  This sort of ``average over conditioning'' argument to get conditional entropy inequalities from unconditional ones is commonly used in this paper.
\end{proof}

\begin{definition}[Mutual information]\label{information-def} If $X: \Omega \to S$, $Y: \Omega \to T$ are random variables, then
  $$ I[ X : Y ] := H[X] + H[Y] - H[X,Y].$$
\end{definition}

\begin{lemma}[Alternative formulae for mutual information]\label{alternative-mutual}
  \uses{information-def,joint-symm,chain-rule}
  With notation as above, we have
$$  I[X : Y] = I[Y:X]$$
$$  I[X : Y] = H[X] - H[X|Y]$$
$$  I[X : Y] = H[Y] - H[Y|X]$$
\end{lemma}

\begin{proof} Immediate from Lemmas \ref{joint-symm}, \ref{chain-rule}.
\end{proof}

\begin{lemma}[Nonnegativity of mutual information]\label{mutual-nonneg}\uses{information-def}  With notation as above, we have $I[X:Y] \geq 0$.
\end{lemma}

\begin{proof}  An application of Jensen.
\end{proof}

\begin{corollary}[Subadditivity]\label{subadditive}\uses{mutual-nonneg,alternative-mutual}  With notation as above, we have $H[X,Y] \leq H[X] + H[Y]$.
\end{corollary}

\begin{proof}  Combine Lemma \ref{mutual-nonneg} with Lemma \ref{alternative-mutual}.
\end{proof}

\begin{corollary}[Conditioning reduces entropy]\label{cond-reduce}\uses{mutual-nonneg,alternative-mutual}  With notation as above, we have $H[X|Y] \leq H[X]$.
\end{corollary}

\begin{proof}  Combine Lemma \ref{mutual-nonneg} with Lemma \ref{alternative-mutual}.
\end{proof}

\begin{corollary}[Submodularity]\label{submodularity}\uses{cond-reduce} With three random variables $X,Y,Z$, one has $H[X|Y,Z] \leq H[X|Z]$.
\end{corollary}

\begin{proof} Apply the ``averaging over conditioning'' argument to Corollary \ref{cond-reduce}.
\end{proof}

\begin{corollary}[Alternate form of submodularity]\label{alt-submodularity}\uses{submodularity,chain-rule} With three random variables $X,Y,Z$, one has
  $$ H[X,Y,Z] + H[Z] \leq H[X,Z] + H[Y,Z].$$
\end{corollary}

\begin{proof}  Apply Corollary \ref{submodularity} and Lemma \ref{chain-rule}.
\end{proof}

\begin{definition}[Independent random variables]\label{independent-def}
Two random variables $X: \Omega \to S$ and $Y: \Omega \to T$ are independent if $P[ X = s \wedge Y = t] = P[X=s] P[Y=t]$ for all $s \in S, t \in T$.  NOTE: will also need a notion of joint independence of $k$ random variables for any finite $k$.
\end{definition}

\begin{lemma}[Additivity of entropy]\label{add-entropy}\uses{chain-rule,conditional-entropy}  If $X,Y$ are random variables, then $H[X,Y] = H[X] + H[Y]$ if and only if $X,Y$ are independent.
\end{lemma}

\begin{proof} Simplest proof for the ``if'' is to use Lemma \ref{chain-rule} and show that $H[X|Y] = H[X]$ by first showing that $H[X|Y=y] = H[X]$ whenever $P[Y=y]$ is non-zero.  ``only if'' direction will require some converse Jensen.
\end{proof}


\begin{corollary}[Vanishing of mutual information]\label{vanish-entropy}\uses{add-entropy,information-def}  If $X,Y$ are random variables, then $I[X,Y] = 0$ if and only if $X,Y$ are independent.
\end{corollary}

\begin{proof} Immediate from Lemma \ref{add-entropy} and Definition \ref{information-def}.
\end{proof}

\begin{definition}[Conditional mutual information]\label{conditional-mutual-def}\uses{information-def,{condition-event-def}  If $X,Y,Z$ are random variables, with $Z$ $U$-valued, then
  $$ I[X:Y|Z] := \sum_{z \in U} P[Z=z] I[(X|Z=z): (Y|Z=z)].$$
\end{definition}

\begin{lemma}[Nonnegativity of conditional mutual information]\label{conditional-nonneg}  \uses{conditional-mutual-def,submodularity}
If $X,Y,Z$ are random variables, then $I[X:Y|Z] \ge 0$.
\end{lemma}

\begin{proof} Use Definition \ref{conditional-mutual-def} and Lemma \ref{submodularity}.
\end{proof}

\begin{definition}[Conditionally independent random variables]\label{conditional-independent-def}
  Two random variables $X: \Omega \to S$ and $Y: \Omega \to T$ are conditionally independent relative to another random variable $Z: \Omega \to U$ if $P[ X = s \wedge Y = t| Z=u] = P[X=s|Z=u] P[Y=t|Z=u]$ for all $s \in S, t \in T, u \in U$.  (We won't need conditional independence for more variables than this.)
\end{definition}

\begin{lemma}[Vanishing conditional mutual information]\label{conditional-vanish}  \uses{conditional-mutual-def,conditional-independent-def,conditional-nonneg}
  If $X,Y,Z$ are random variables, then $I[X:Y|Z] \ge 0$.
\end{lemma}

\begin{proof} Immediate from Lemma \ref{conditional-nonneg} and Definitions \ref{conditional-mutual-def}, \ref{conditional-independent-def}.
\end{proof}
