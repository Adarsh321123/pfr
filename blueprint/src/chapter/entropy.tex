\chapter{Shannon entropy inequalities}

Random variables in this paper are measurable maps $X : \Omega \to S$ from a probability space $\Omega$ to a finite set $S$ (equipped with the discrete random variable), and called $S$-valued random variables.

\begin{definition}[Entropy]
  \label{entropy-def}
  \lean{entropy}
  \leanok
  If $X$ is an $S$-valued random variable, the entropy $H[X]$ of $X$ is defined
  $$ H[X] := \sum_{s \in S} P[X=x] \log \frac{1}{P[X=x]}$$
  with the convention that $0 \log \frac{1}{0} = 0$.
\end{definition}

\begin{lemma}[Entropy and relabeling]\label{relabeled-entropy} \uses{entropy-def}  If $X: \Omega \to S$ and $Y: \Omega \to T$ are random variables, and $Y = f(X)$ almost surely for some injection $f: S \to T$, then $H[X] = H[Y]$.
\end{lemma}

\begin{proof} Expand out both entropies and rearrange.
\end{proof}

\begin{lemma}[Jensen bound]\label{jensen-bound}
\uses{entropy-def}
\lean{entropy_le_log}
\leanok
If $X$ is an $S$-valued random variable, then $H[X] \leq \log |X|$.
\end{lemma}

\begin{proof}
  This is a direct consequence of Jensen's inequality applied to the concave function $x \mapsto x \log \frac{1}{x}$.
\end{proof}

\begin{lemma}[Entropy of uniform random variable]\label{uniform-entropy}
  \uses{entropy-def}
  \lean{entropy_of_uniform}
  \leanok
If $X$ is a uniformly distributed $S$-valued random variable, then $H[X] = \log |X|$.
\end{lemma}

\begin{proof} Direct computation.
\end{proof}

\begin{lemma}[Bounded entropy implies concentration]\label{bound-conc}
  \uses{entropy-def}
  If $X$ is an $S$-valued random variable, then there exists $s \in S$ such that $P[X=s] \geq \exp(-H[X])$.
\end{lemma}

\begin{proof}  We have
$$ H[X] = \sum_{s \in S} P[X=s] \log \frac{1}{P[X=s]} \geq \min_{s \in S} \log \frac{1}{P[X=s]}$$
and the claim follows.
\end{proof}

\begin{definition}[Pair of random variables]\label{pair-def}
  \lean{pair}
  \leanok
Given an $S$-random variable $X: \Omega \to S$ and a $T$-valued random variable $Y: \Omega \to T$, the pair $\langle X,Y \rangle: \Omega \to S \times T$ is defined by $\langle X, Y \rangle: \omega \mapsto (X(\omega), Y(\omega))$.
\end{definition}

\begin{definition}[Conditioned event]\label{condition-event-def}  If $X: \Omega \to S$ is an $S$-valued random variable and $E$ is an event in $\Omega$, then the conditioned event $(X|E)$ is defined to be the same random variable as $X$, but now the ambient probability measure has been conditioned to $E$.
\end{definition}

Note: it may happen that $E$ has zero measure.  In which case, the ambient probability measure should be replaced with a zero measure.  (There will be support in the project for ``probability measures with zero'' to deal with this contingency.  Fortunately, with a zero probability measure all entropies are zero, so entropy inequalities are trivially true in this case.)

\begin{definition}[Conditional entropy]\label{conditional-entropy-def}
  \uses{entropy-def}
  \uses{condition-event-def}  If $X: \Omega \to S$ and $Y: \Omega \to T$ are random variables, the conditional entropy $H[X|Y]$ is defined as
  $$ H[X|Y] := \sum_{y \in Y} P[Y = y] H[(X | Y=y)].$$
\end{definition}

\begin{lemma}[Chain rule]\label{chain-rule}
  \uses{conditional-entropy-def} \uses{pair-def}
  If $X: \Omega \to S$ and $Y: \Omega \to T$ are random variables, then
  $$ H[ \langle X,Y \rangle ] = H[Y] + H[X|Y].$$
\end{lemma}

\begin{proof} Direct computation, I guess?
\end{proof}

\begin{lemma}[Conditional chain rule]\label{conditional-chain-rule} \uses{chain-rule}
  If $X: \Omega \to S$, $Y: \Omega \to T$, $Z: \Omega \to U$ are random variables, then
$$ H[ \langle X,Y \rangle | Z ] = H[Y | Z] + H[X|Y, Z].$$
\end{lemma}

\begin{proof}  For each $z \in U$, we can apply Lemma \ref{chain-rule} to the random variables $(X|Z=z)$ and $(Y|Z=z)$ to obtain
$$ H[ \langle (X|Z=z),(Y|Z=z) \rangle ] = H[Y|Z=z] + H[(X|Z=z)|(Y|Z=z)].$$
Now multiply by $P[Z=z]$ and sum.  Some helper lemmas may be needed to get to the form above.  This sort of ``average over conditioning'' argument to get conditional entropy inequalities from unconditional ones is commonly used in this paper.
\end{proof}

\begin{definition}[Mutual information]\label{information-def} If $X: \Omega \to S$, $Y: \Omega \to T$ are random variables, then
  $$ I[ X : Y ] := H[X] + H[Y] - H[X,Y].$$
\end{definition}

Should provide some small helper lemmas:
